{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imports\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "#rd kit\n",
    "from rdkit.Chem import Draw, AllChem, MACCSkeys\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "#sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "#importing the preprocessed embedds\n",
    "embedds = dict( np.load('protein.npz', mmap_mode='r' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DataSet from xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with predefined column names\n",
    "DataSet = pd.DataFrame(columns=['name', 'smiles','molecule', 'fingerprint', 'target', 'SeqVec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse XML\n",
    "root = ET.parse('full_database.xml').getroot()\n",
    "neg_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter for stats\n",
    "counter = 0\n",
    "\n",
    "#for all drugs in databank\n",
    "for i in range(len(root)):\n",
    "    \n",
    "    #if drug is a \"small molecule\"\n",
    "    if (root[i].attrib.get('type') == \"small molecule\"):\n",
    "        \n",
    "        Datasetentry = []\n",
    "        #tnumber counts the number of targets the drug has\n",
    "        tarnumber = 0\n",
    "        \n",
    "        #getting the drugs name\n",
    "        Datasetentry.append(root[i].find('{http://www.drugbank.ca}name').text)  \n",
    "        \n",
    "         \n",
    "        #go thru properties searching for the smiles\n",
    "        smiles = ''\n",
    "        for property in root[i].find('{http://www.drugbank.ca}calculated-properties'):\n",
    "            if(property[0].text == \"SMILES\"):\n",
    "                smiles = property.find('{http://www.drugbank.ca}value').text\n",
    "                Datasetentry.append(smiles)\n",
    "        \n",
    "        #add \"X\" if no smiles is found\n",
    "        if len(Datasetentry) == 1:\n",
    "            Datasetentry.append(\"X\")\n",
    "        \n",
    "        \n",
    "        #add empty cell for molecule object\n",
    "        Datasetentry.append(\"0\")\n",
    "        \n",
    "        #add empty cell for fingerprint object\n",
    "        Datasetentry.append(\"0\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #go thru targets to find their id\n",
    "        try:\n",
    "            #if the drug has a target its id is added to the dataframe\n",
    "            if root[i].find('{http://www.drugbank.ca}targets').find('{http://www.drugbank.ca}target') != None:\n",
    "                tars = root[i].find('{http://www.drugbank.ca}targets')\n",
    "                for target in tars:\n",
    "                    tar = target.find('{http://www.drugbank.ca}polypeptide')\n",
    "                    x = tar.get('id')\n",
    "                    Datasetentry.append(x)\n",
    "                    Datasetentry.append(\"0\")\n",
    "                    DataSet.loc[len(DataSet)] = Datasetentry\n",
    "                    Datasetentry = Datasetentry[:-2]\n",
    "                    counter = counter + 1\n",
    "                    tarnumber = tarnumber + 1\n",
    "            #if it hasn't an empty cell is added instead\n",
    "            else:\n",
    "                Datasetentry.append(\"0\")\n",
    "                Datasetentry.append(\"0\")\n",
    "                DataSet.loc[len(DataSet)] = Datasetentry\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        #adding the smiles of drugs that have between 3 and 10 targets to a list\n",
    "        if tarnumber>2 and tarnumber<11:\n",
    "            neg_targets.append(smiles)\n",
    "        \n",
    "        \n",
    "print(len(DataSet))\n",
    "print(counter)\n",
    "print(len(neg_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataset so we dont have to run the timeconsuming parts\n",
    "pd.to_pickle(DataSet, \"DataSet_SVM_woreduction\", compression='infer', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset from pickle\n",
    "DataSet = pd.read_pickle(\"DataSet2_fpsize1k\", compression='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction of proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "identifiers50 = []\n",
    "with open('protein50.fasta') as fasta_file:  # Will close handle cleanly\n",
    "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "        identifiers50.append(seq_record.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protIds50 = []\n",
    "for i in identifiers50:\n",
    "    protIds50.append(i[i.find(\"|\")+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction of drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClusterFps(fps,cutoff=0.5):\n",
    "    from rdkit import DataStructs\n",
    "    from rdkit.ML.Cluster import Butina\n",
    "\n",
    "    # first generate the distance matrix:\n",
    "    dists = []\n",
    "    nfps = len(fps)\n",
    "    for i in range(1,nfps):\n",
    "        sims = DataStructs.BulkTanimotoSimilarity(fps[i],fps[:i])\n",
    "        dists.extend([1-x for x in sims])\n",
    "\n",
    "    # now cluster the data:\n",
    "    cs = Butina.ClusterData(dists,nfps,cutoff,isDistData=True)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all fingerprints\n",
    "fingerprints = []\n",
    "for index, row in DataSet.iterrows():\n",
    "    if(DataSet.at[index, 'name']==0):\n",
    "        continue\n",
    "    else:\n",
    "        if len(DataSet.at[index, 'fingerprint'])>4:\n",
    "            fingerprints.append(DataSet.at[index, 'fingerprint'])\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering the fingerprints\n",
    "clusters=ClusterFps(fingerprints,cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_drugs = []\n",
    "for i in range(len(clusters)):\n",
    "    cl = clusters[i]\n",
    "    id = cl[0]\n",
    "    reduced_drugs.append(fingerprints[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = []\n",
    "for index, row in DataSet.iterrows():\n",
    "    if DataSet.at[index, 'fingerprint'] in reduced_drugs:\n",
    "        if DataSet.at[index, 'target'] in protIds50:\n",
    "            x=DataSet.at[index,'fingerprint']\n",
    "            z=embedds[DataSet.at[index, 'target']]\n",
    "            ml_data.append(np.concatenate((x,z), axis=0))\n",
    "\n",
    "for entry in ml_data:\n",
    "    for number in entry:\n",
    "        number = float(number)\n",
    "        \n",
    "np.random.shuffle(ml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the ml_data as file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "savez_compressed('data.npz',ml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data form file\n",
    "from numpy import load\n",
    "dict_data = load('data.npz')\n",
    "ml_data = dict_data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for entry in ml_data:\n",
    "    if(len(entry)==2048):\n",
    "        X.append(entry)\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size=0.15)\n",
    "print(\"Dataset sizes: \\nWhole set: {}\\nTraining Set: {}\\nTest Set: {}\".format(len(y), len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding negatives to test data\n",
    "len_test = len(y_test)\n",
    "for i in range(len_test):\n",
    "    fingerprint = random.choice(fingerprints)\n",
    "    y_test.append(0)\n",
    "    tgt = random.choice(protIds50)\n",
    "    seqvec = embedds[tgt]\n",
    "    temp = np.concatenate((fingerprint,seqvec), axis=0)\n",
    "    for number in temp:\n",
    "        number=float(number)\n",
    "    X_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding five negatives to the training set\n",
    "for i in range(5):\n",
    "    fingerprint= random.choice(fingerprints)\n",
    "    y_train.append(0)\n",
    "    tgt = random.choice(protIds50)\n",
    "    seqvec = embedds[tgt]\n",
    "    temp = np.concatenate((fingerprint,seqvec),axis=0)\n",
    "    for number in temp:\n",
    "        number=float(number)\n",
    "    X_train.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation to optimize hyperparameters\n",
    "\n",
    "# Define cross-validation object\n",
    "cv = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "# Define predictor\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(probability=True)\n",
    "\n",
    "# Define parameters we want to optimize and values we want to test\n",
    "# Here, we test different activation functions\n",
    "params = { 'decision_function_shape': ['ovo', 'ovr']}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator = classifier, cv = cv, param_grid = params, \n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Analyse results\n",
    "\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator and assess performance on the test set\n",
    "\n",
    "# Calculate predictions\n",
    "best_classifier = grid.best_estimator_\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "pred_score = best_classifier.score(X_test, y_test)\n",
    "\n",
    "# Calculate confusion matrix (showing tp, fp, tn, fn)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Acc: {}'.format(round(pred_score, 3)))\n",
    "predictions=y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the AUC for the modell\n",
    "\n",
    "probas = best_classifier.predict_proba(X_test)\n",
    "proba_predictions = []\n",
    "for entry in probas:\n",
    "    proba_predictions.append(entry[0])\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, proba_predictions)\n",
    "print(\"AUC: {}\".format(auc))\n",
    "AUCs = []\n",
    "for i in range(len(y_test)):\n",
    "    truths = []\n",
    "    preds = []\n",
    "    for j in range(len(y_test)):\n",
    "        pick = random.randint(0,len(y_test)-1)\n",
    "        truth = y_test[pick]\n",
    "        pred = proba_predictions[pick]\n",
    "        truths.append(truth)\n",
    "        preds.append(pred)\n",
    "    auc2 = roc_auc_score(truths,preds)\n",
    "    AUCs.append(auc2)\n",
    "std_auc = np.std(AUCs)\n",
    "print(std_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_errs():\n",
    "    iterations = len(y_test)\n",
    "    overall_accs = []\n",
    "    precisions = []\n",
    "    neg_precs = []\n",
    "    recalls = []\n",
    "    neg_covs = []\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for i in range(iterations):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(iterations):\n",
    "            pick = random.randint(0,len(y_test)-1)\n",
    "            truth = y_test[pick]\n",
    "            pred = predictions[pick]\n",
    "            if truth == 1:\n",
    "                if pred == 1:\n",
    "                    tp = tp +1\n",
    "                else:\n",
    "                    fn = fn +1\n",
    "            else:\n",
    "                if pred == 1:\n",
    "                    fp = fp +1\n",
    "                else:\n",
    "                    tn = tn +1\n",
    "        #formulas of performance mesurements\n",
    "        precision = tp/(tp+fp)\n",
    "        precisions.append(precision)\n",
    "        neg_prec = tn/(tn+fn)\n",
    "        neg_precs.append(neg_prec)\n",
    "        recall = tp/(tp+fn)\n",
    "        recalls.append(recall)\n",
    "        neg_cov = tn/(tn+fp)\n",
    "        neg_covs.append(neg_cov)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        f1s.append(f1)\n",
    "        overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "        overall_accs.append(overall_acc)\n",
    "        mcc = matthews_corrcoef(y_test,predictions)\n",
    "        mccs.append(mcc)\n",
    "    #calculate standard deviation of the performance mesurements\n",
    "    std_prec = np.std(precisions)\n",
    "    std_neg_prec = np.std(neg_precs)\n",
    "    std_recall = np.std(recalls)\n",
    "    std_neg_cov = np.std(neg_covs)\n",
    "    std_f1 = np.std(f1s)\n",
    "    std_overall_acc = np.std(overall_accs)\n",
    "    std_mcc = np.std(mccs)\n",
    "    print(\"std_prec: {}\\nstd_neg_prec: {}\\nstd_recall: {}\\nstd_neg_cov: {}\\nstd_f1: {}\\nstd_overall_acc: {}\\nstd_mcc: {}\".format(std_prec, std_neg_prec, std_recall, std_neg_cov, std_f1, std_overall_acc, std_mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(predictions)-1):\n",
    "    pred = i\n",
    "    if y_test[i] == 1:\n",
    "        if pred == 1:\n",
    "            tp = tp+1\n",
    "        else:\n",
    "            fn = fn+1\n",
    "    else:\n",
    "        if pred == 1:\n",
    "            fp = fp + 1\n",
    "        else:\n",
    "            tn = tn + 1\n",
    "print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\".\n",
    "      format(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of the predictions\n",
    "tp = 1\n",
    "tn = 685\n",
    "fp = 1\n",
    "fn = 685\n",
    "\n",
    "#calculating performance scores\n",
    "precision = tp/(tp+fp)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "\n",
    "neg_prec = tn/(tn+fn)\n",
    "print(\"negative Precision:\")\n",
    "print(neg_prec)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "\n",
    "neg_cov = tn/(tn+fp)\n",
    "print(\"negative coverage:\")\n",
    "print(neg_cov)\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1-score:\")\n",
    "print(f1)\n",
    "\n",
    "overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "print(\"overall accuracy:\")\n",
    "print(overall_acc)\n",
    "\n",
    "print(\"MCC:\")\n",
    "print(matthews_corrcoef(y_test,predictions))\n",
    "\n",
    "calc_std_errs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the distirbution of classes in all the data\n",
    "#data is all the available data\n",
    "def get_distrib(data):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for entry in data:\n",
    "        if entry[1] == 1:\n",
    "            positive = positive + 1\n",
    "        else:\n",
    "            negative = negative + 1\n",
    "    total = positive + negative\n",
    "    #scaling the data into percentages\n",
    "    posper = (positive/total) * 100\n",
    "    return posper\n",
    "\n",
    "\n",
    "\n",
    "#distribution is the amount of times the dominant class appears out of 100 entrys\n",
    "def ZeroRuleBaseline(distribution):\n",
    "    #a random number between 1 and 100 is generated\n",
    "    tempPred = random.randint(1,101)\n",
    "    if tempPred <= distribution:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_errs_baseline():\n",
    "    iterations = len(y_test)\n",
    "    overall_accs = []\n",
    "    precisions = []\n",
    "    neg_precs = []\n",
    "    recalls = []\n",
    "    neg_covs = []\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for i in range(iterations):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(iterations):\n",
    "            pick = random.randint(0,len(y_test)-1)\n",
    "            truth = y_test[pick]\n",
    "            pred = baseline_predicts[pick]\n",
    "            if truth == 1:\n",
    "                if pred == 1:\n",
    "                    tp = tp +1\n",
    "                else:\n",
    "                    fn = fn +1\n",
    "            else:\n",
    "                if pred == 1:\n",
    "                    fp = fp +1\n",
    "                else:\n",
    "                    tn = tn +1\n",
    "        #formulas of performance mesurements\n",
    "        precision = tp/(tp+fp)\n",
    "        precisions.append(precision)\n",
    "        neg_prec = tn/(tn+fn)\n",
    "        neg_precs.append(neg_prec)\n",
    "        recall = tp/(tp+fn)\n",
    "        recalls.append(recall)\n",
    "        neg_cov = tn/(tn+fp)\n",
    "        neg_covs.append(neg_cov)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        f1s.append(f1)\n",
    "        overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "        overall_accs.append(overall_acc)\n",
    "        mcc = matthews_corrcoef(y_test,baseline_predicts)\n",
    "        mccs.append(mcc)\n",
    "    #calculate standard deviation of the performance mesurements\n",
    "    std_prec = np.std(precisions)\n",
    "    std_neg_prec = np.std(neg_precs)\n",
    "    std_recall = np.std(recalls)\n",
    "    std_neg_cov = np.std(neg_covs)\n",
    "    std_f1 = np.std(f1s)\n",
    "    std_overall_acc = np.std(overall_accs)\n",
    "    std_mcc = np.std(mccs)\n",
    "    print(\"std_prec: {}\\nstd_neg_prec: {}\\nstd_recall: {}\\nstd_neg_cov: {}\\nstd_f1: {}\\nstd_overall_acc: {}\\nstd_mcc: {}\".format(std_prec, std_neg_prec, std_recall, std_neg_cov, std_f1, std_overall_acc, std_mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the performance of the baseline prediction\n",
    "dist = get_distrib(ml_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "baseline_predicts = []\n",
    "for i in range(len(y_test)):\n",
    "    pred = ZeroRuleBaseline(dist)\n",
    "    baseline_predicts.append(pred)\n",
    "    if y_test[i] == 1:\n",
    "        if pred == 1:\n",
    "            tp = tp +1\n",
    "        else:\n",
    "            fn = fn +1\n",
    "    else:\n",
    "        if pred == 1:\n",
    "            fp = fp +1\n",
    "        else:\n",
    "            tn = tn +1\n",
    "print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\".format(tp, tn, fp, fn))\n",
    "#calculating performance scores\n",
    "precision = tp/(tp+fp)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "\n",
    "neg_prec = tn/(tn+fn)\n",
    "print(\"negative Precision:\")\n",
    "print(neg_prec)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "\n",
    "neg_cov = tn/(tn+fp)\n",
    "print(\"negative coverage:\")\n",
    "print(neg_cov)\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1-score:\")\n",
    "print(f1)\n",
    "\n",
    "overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "print(\"overall accuracy:\")\n",
    "print(overall_acc)\n",
    "\n",
    "print(\"MCC:\")\n",
    "print(matthews_corrcoef(y_test,baseline_predicts))\n",
    "\n",
    "calc_std_errs_baseline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbl",
   "language": "python",
   "name": "pbl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
