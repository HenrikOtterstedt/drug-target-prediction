{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imports\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "#rd kit\n",
    "from rdkit.Chem import Draw, AllChem, MACCSkeys\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "#sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "#importing the preprocessed embedds\n",
    "embedds = dict( np.load('protein.npz', mmap_mode='r' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset from .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame with predefined column names\n",
    "DataSet = pd.DataFrame(columns=['name', 'smiles','molecule', 'fingerprint', 'target', 'SeqVec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse XML\n",
    "root = ET.parse('full_database.xml').getroot()\n",
    "neg_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter for stats\n",
    "counter = 0\n",
    "\n",
    "#for all drugs in databank\n",
    "for i in range(len(root)):\n",
    "    \n",
    "    #if drug is a \"small molecule\"\n",
    "    if (root[i].attrib.get('type') == \"small molecule\"):\n",
    "        \n",
    "        Datasetentry = []\n",
    "        #tnumber counts the number of targets the drug has\n",
    "        tarnumber = 0\n",
    "        \n",
    "        #getting the drugs name\n",
    "        Datasetentry.append(root[i].find('{http://www.drugbank.ca}name').text)  \n",
    "        \n",
    "         \n",
    "        #go thru properties searching for the smiles\n",
    "        smiles = ''\n",
    "        for property in root[i].find('{http://www.drugbank.ca}calculated-properties'):\n",
    "            if(property[0].text == \"SMILES\"):\n",
    "                smiles = property.find('{http://www.drugbank.ca}value').text\n",
    "                Datasetentry.append(smiles)\n",
    "        \n",
    "        #add \"X\" if no smiles is found\n",
    "        if len(Datasetentry) == 1:\n",
    "            Datasetentry.append(\"X\")\n",
    "        \n",
    "        \n",
    "        #add empty cell for molecule object\n",
    "        Datasetentry.append(\"0\")\n",
    "        \n",
    "        #add empty cell for fingerprint object\n",
    "        Datasetentry.append(\"0\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #go thru targets to find their id\n",
    "        try:\n",
    "            #if the drug has a target its id is added to the dataframe\n",
    "            if root[i].find('{http://www.drugbank.ca}targets').find('{http://www.drugbank.ca}target') != None:\n",
    "                tars = root[i].find('{http://www.drugbank.ca}targets')\n",
    "                for target in tars:\n",
    "                    tar = target.find('{http://www.drugbank.ca}polypeptide')\n",
    "                    x = tar.get('id')\n",
    "                    Datasetentry.append(x)\n",
    "                    Datasetentry.append(\"0\")\n",
    "                    DataSet.loc[len(DataSet)] = Datasetentry\n",
    "                    Datasetentry = Datasetentry[:-2]\n",
    "                    counter = counter + 1\n",
    "                    tarnumber = tarnumber + 1\n",
    "            #if it hasn't an empty cell is added instead\n",
    "            else:\n",
    "                Datasetentry.append(\"0\")\n",
    "                Datasetentry.append(\"0\")\n",
    "                DataSet.loc[len(DataSet)] = Datasetentry\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        #adding the smiles of drugs that have between 3 and 10 targets to a list\n",
    "        if tarnumber>2 and tarnumber<11:\n",
    "            neg_targets.append(smiles)\n",
    "        \n",
    "        \n",
    "print(len(DataSet))\n",
    "print(counter)\n",
    "print(len(neg_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding as many negative interactions as there are positive\n",
    "datalen = len(DataSet)\n",
    "for i in range(datalen):\n",
    "    x = []\n",
    "    #adding a 0 as the drugs name so we can identify the made up negative interactions\n",
    "    x.append(0)\n",
    "    #selecting a random smiles for the interaction\n",
    "    smi = random.choice(neg_targets)\n",
    "    x.append(smi)\n",
    "    #molecule\n",
    "    x.append(\"0\")\n",
    "    #fingerprint\n",
    "    x.append(\"0\")\n",
    "    #randoming a seqvec which doesnt have a previous entry for the smiles\n",
    "    tgt = random.choice(list(embedds))\n",
    "    for index, row in DataSet.iterrows():\n",
    "        #ignoring ones that have a previously entered interaction\n",
    "        if DataSet.at[index, 'smiles'] == smi:\n",
    "            if DataSet.at[index, 'target'] == tgt:\n",
    "                #duplicatates get marked with a 0 as target so they get ignored furhter on\n",
    "                x.append(0)\n",
    "                continue\n",
    "                \n",
    "    if len(x) == 4:\n",
    "        x.append(tgt)\n",
    "    #print(x)\n",
    "    #Seqvec\n",
    "    x.append(\"0\")\n",
    "    #adding the interaction to the dataset\n",
    "    DataSet.loc[len(DataSet)] = x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in the seqvec\n",
    "for index, row in DataSet.iterrows():\n",
    "    try:\n",
    "        DataSet.at[index, 'SeqVec'] = embedds[row['target']]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create molecule from smiles\n",
    "for index, row in DataSet.iterrows():\n",
    "    DataSet.at[index, 'molecule'] = Chem.MolFromSmiles(row['smiles'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fingerprint from molecule\n",
    "   \n",
    "for index, row in DataSet.iterrows():\n",
    "    try:\n",
    "        DataSet.at[index, 'fingerprint'] = rdkit.Chem.rdmolops.RDKFingerprint([index, 'moelcule'],fpSize=1024)\n",
    "        #entries[0] = Chem.MolFromSmiles(entries.at['smiles'])\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataset so we dont have to run the timeconsuming parts\n",
    "pd.to_pickle(DataSet, \"DataSet2\", compression='infer', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = []\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "for index, row in DataSet.iterrows():\n",
    "    temp = []\n",
    "    #if there is no fingerprint the entry gets ignored\n",
    "    if len(DataSet.at[index, 'fingerprint']) > 3:\n",
    "        \n",
    "        #if a fingerprint and seqvec are there the vectors get concatinated and added to a list wihilst denoting that they connect\n",
    "        if len(DataSet.at[index, 'SeqVec']) > 5:\n",
    "            #checking for the made up negative interactions\n",
    "            if DataSet.at[index, 'name'] == 0:\n",
    "                temp.append(np.concatenate((DataSet.at[index, 'fingerprint'], row['SeqVec']), axis=0))\n",
    "                temp.append(0)\n",
    "                count0 = count0 + 1\n",
    "            else:    \n",
    "                temp.append(np.concatenate((DataSet.at[index, 'fingerprint'], row['SeqVec']), axis=0))\n",
    "                temp.append(1)\n",
    "                count1 = count1 + 1\n",
    "            \n",
    "        #if there is no target found the entry gets ignored\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        continue\n",
    "    ml_data.append(temp)\n",
    "\n",
    "\n",
    "#converting the vector entierly to floats\n",
    "for entry in ml_data:\n",
    "    for number in entry[0]:\n",
    "        number = float(number)\n",
    "        \n",
    "        \n",
    "#shuffleing the array\n",
    "np.random.shuffle(ml_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the distirbution of classes in all the data\n",
    "#data is all the available data\n",
    "def get_distrib(data):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for entry in data:\n",
    "        if entry[1] == 1:\n",
    "            positive = positive + 1\n",
    "        else:\n",
    "            negative = negative + 1\n",
    "    total = positive + negative\n",
    "    #scaling the data into percentages\n",
    "    posper = (positive/total) * 100\n",
    "    return posper\n",
    "\n",
    "\n",
    "\n",
    "#distribution is the amount of times the dominant class appears out of 100 entrys\n",
    "def ZeroRuleBaseline(distribution):\n",
    "    #a random number between 1 and 100 is generated\n",
    "    tempPred = random.randint(1,101)\n",
    "    if tempPred <= distribution:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalising X and y unsing the ml_data\n",
    "X = []\n",
    "y = []\n",
    "for entry in ml_data:\n",
    "    if(len(entry[0])==2048):\n",
    "        X.append(entry[0])\n",
    "        y.append(entry[1])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "print(\"Dataset sizes:\\nWhole set: {}\\nTraining Set: {}\\nTest Set: {}\".\n",
    "      format(len(y), len(y_train), len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation to optimize hyperparameters\n",
    "\n",
    "# Define cross-validation object\n",
    "cv = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "# Define predictor\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(probability=True)\n",
    "\n",
    "# Define parameters we want to optimize and values we want to test\n",
    "# Here, we test different activation functions\n",
    "params = { 'decision_function_shape': ['ovo']}\n",
    "\n",
    "# Perform grid search\n",
    "grid = GridSearchCV(estimator = classifier, cv = cv, param_grid = params, \n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Analyse results\n",
    "\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator and assess performance on the test set\n",
    "\n",
    "# Calculate predictions\n",
    "best_classifier = grid.best_estimator_\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "pred_score = best_classifier.score(X_test, y_test)\n",
    "\n",
    "# Calculate confusion matrix (showing tp, fp, tn, fn)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Acc: {}'.format(round(pred_score, 3)))\n",
    "predictions=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the AUC for the modell\n",
    "\n",
    "probas = best_classifier.predict_proba(X_test)\n",
    "proba_predictions = []\n",
    "for entry in probas:\n",
    "    proba_predictions.append(entry[0])\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, proba_predictions)\n",
    "print(\"AUC: {}\".format(auc))\n",
    "AUCs = []\n",
    "for i in range(len(y_test)):\n",
    "    truths = []\n",
    "    preds = []\n",
    "    for j in range(len(y_test)):\n",
    "        pick = random.randint(0,len(y_test)-1)\n",
    "        truth = y_test[pick]\n",
    "        pred = proba_predictions[pick]\n",
    "        truths.append(truth)\n",
    "        preds.append(pred)\n",
    "    auc2 = roc_auc_score(truths,preds)\n",
    "    AUCs.append(auc2)\n",
    "std_auc = np.std(AUCs)\n",
    "print(std_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_errs():\n",
    "    iterations = len(y_test)\n",
    "    overall_accs = []\n",
    "    precisions = []\n",
    "    neg_precs = []\n",
    "    recalls = []\n",
    "    neg_covs = []\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for i in range(iterations):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(iterations):\n",
    "            pick = random.randint(0,len(y_test)-1)\n",
    "            truth = y_test[pick]\n",
    "            pred = predictions[pick]\n",
    "            if truth == 1:\n",
    "                if pred == 1:\n",
    "                    tp = tp +1\n",
    "                else:\n",
    "                    fn = fn +1\n",
    "            else:\n",
    "                if pred == 1:\n",
    "                    fp = fp +1\n",
    "                else:\n",
    "                    tn = tn +1\n",
    "        #formulas of performance mesurements\n",
    "        precision = tp/(tp+fp)\n",
    "        precisions.append(precision)\n",
    "        neg_prec = tn/(tn+fn)\n",
    "        neg_precs.append(neg_prec)\n",
    "        recall = tp/(tp+fn)\n",
    "        recalls.append(recall)\n",
    "        neg_cov = tn/(tn+fp)\n",
    "        neg_covs.append(neg_cov)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        f1s.append(f1)\n",
    "        overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "        overall_accs.append(overall_acc)\n",
    "        mcc = matthews_corrcoef(y_test,predictions)\n",
    "        mccs.append(mcc)\n",
    "    #calculate standard deviation of the performance mesurements\n",
    "    std_prec = np.std(precisions)\n",
    "    std_neg_prec = np.std(neg_precs)\n",
    "    std_recall = np.std(recalls)\n",
    "    std_neg_cov = np.std(neg_covs)\n",
    "    std_f1 = np.std(f1s)\n",
    "    std_overall_acc = np.std(overall_accs)\n",
    "    std_mcc = np.std(mccs)\n",
    "    print(\"std_prec: {}\\nstd_neg_prec: {}\\nstd_recall: {}\\nstd_neg_cov: {}\\nstd_f1: {}\\nstd_overall_acc: {}\\nstd_mcc: {}\".format(std_prec, std_neg_prec, std_recall, std_neg_cov, std_f1, std_overall_acc, std_mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of the predictions\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    pred = predictions[i]\n",
    "    if y_test[i] == 1:\n",
    "        if pred == 1:\n",
    "            tp = tp+1\n",
    "        else:\n",
    "            fn = fn+1\n",
    "    else:\n",
    "        if pred == 1:\n",
    "            fp = fp + 1\n",
    "        else:\n",
    "            tn = tn + 1\n",
    "print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\".\n",
    "      format(tp, tn, fp, fn))\n",
    "\n",
    "#calculating performance scores\n",
    "precision = tp/(tp+fp)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "\n",
    "neg_prec = tn/(tn+fn)\n",
    "print(\"negative Precision:\")\n",
    "print(neg_prec)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "\n",
    "neg_cov = tn/(tn+fp)\n",
    "print(\"negative coverage:\")\n",
    "print(neg_cov)\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1-score:\")\n",
    "print(f1)\n",
    "\n",
    "overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "print(\"overall accuracy:\")\n",
    "print(overall_acc)\n",
    "\n",
    "print(\"MCC:\")\n",
    "print(matthews_corrcoef(y_test,predictions))\n",
    "\n",
    "calc_std_errs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_errs_baseline():\n",
    "    iterations = len(y_test)\n",
    "    overall_accs = []\n",
    "    precisions = []\n",
    "    neg_precs = []\n",
    "    recalls = []\n",
    "    neg_covs = []\n",
    "    f1s = []\n",
    "    mccs = []\n",
    "    for i in range(iterations):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(iterations):\n",
    "            pick = random.randint(0,len(y_test)-1)\n",
    "            truth = y_test[pick]\n",
    "            pred = baseline_predicts[pick]\n",
    "            if truth == 1:\n",
    "                if pred == 1:\n",
    "                    tp = tp +1\n",
    "                else:\n",
    "                    fn = fn +1\n",
    "            else:\n",
    "                if pred == 1:\n",
    "                    fp = fp +1\n",
    "                else:\n",
    "                    tn = tn +1\n",
    "        #formulas of performance mesurements\n",
    "        precision = tp/(tp+fp)\n",
    "        precisions.append(precision)\n",
    "        neg_prec = tn/(tn+fn)\n",
    "        neg_precs.append(neg_prec)\n",
    "        recall = tp/(tp+fn)\n",
    "        recalls.append(recall)\n",
    "        neg_cov = tn/(tn+fp)\n",
    "        neg_covs.append(neg_cov)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        f1s.append(f1)\n",
    "        overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "        overall_accs.append(overall_acc)\n",
    "        mcc = matthews_corrcoef(y_test,baseline_predicts)\n",
    "        mccs.append(mcc)\n",
    "    #calculate standard deviation of the performance mesurements\n",
    "    std_prec = np.std(precisions)\n",
    "    std_neg_prec = np.std(neg_precs)\n",
    "    std_recall = np.std(recalls)\n",
    "    std_neg_cov = np.std(neg_covs)\n",
    "    std_f1 = np.std(f1s)\n",
    "    std_overall_acc = np.std(overall_accs)\n",
    "    std_mcc = np.std(mccs)\n",
    "    print(\"std_prec: {}\\nstd_neg_prec: {}\\nstd_recall: {}\\nstd_neg_cov: {}\\nstd_f1: {}\\nstd_overall_acc: {}\\nstd_mcc: {}\".format(std_prec, std_neg_prec, std_recall, std_neg_cov, std_f1, std_overall_acc, std_mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the performance of the baseline prediction\n",
    "dist = get_distrib(ml_data)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "baseline_predicts = []\n",
    "for i in range(len(y_test)):\n",
    "    pred = ZeroRuleBaseline(dist)\n",
    "    baseline_predicts.append(pred)\n",
    "    if y_test[i] == 1:\n",
    "        if pred == 1:\n",
    "            tp = tp +1\n",
    "        else:\n",
    "            fn = fn +1\n",
    "    else:\n",
    "        if pred == 1:\n",
    "            fp = fp +1\n",
    "        else:\n",
    "            tn = tn +1\n",
    "print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\".format(tp, tn, fp, fn))\n",
    "#calculating performance scores\n",
    "precision = tp/(tp+fp)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "\n",
    "neg_prec = tn/(tn+fn)\n",
    "print(\"negative Precision:\")\n",
    "print(neg_prec)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "\n",
    "neg_cov = tn/(tn+fp)\n",
    "print(\"negative coverage:\")\n",
    "print(neg_cov)\n",
    "\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1-score:\")\n",
    "print(f1)\n",
    "\n",
    "overall_acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "print(\"overall accuracy:\")\n",
    "print(overall_acc)\n",
    "\n",
    "print(\"MCC:\")\n",
    "print(matthews_corrcoef(y_test,baseline_predicts))\n",
    "\n",
    "calc_std_errs_baseline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset from pickle\n",
    "DataSet = pd.read_pickle(\"DataSet2_fpsize1k\", compression='infer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbl",
   "language": "python",
   "name": "pbl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
